# -*- coding: utf-8 -*-
"""Simple Conversational chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X2zntDFotQI4vl0mHCc4euhLMv71i5On
"""

pip install torch

pip install transformers

pip install streamlit

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import streamlit as st

# Load the pre-trained GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Function to generate chatbot responses
def generate_response(user_input):
    inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors="pt")
    outputs = model.generate(
        inputs,
        max_length=150,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        temperature=0.7
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Basic chatbot loop in terminal
def chatbot_terminal():
    print("Chatbot initialized! Type 'exit' to end the conversation.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break
        response = generate_response(user_input)
        print(f"Bot: {response}")

# Streamlit web interface
def chatbot_web():
    st.title("GPT-2 Chatbot")
    user_input = st.text_input("You:")
    if user_input:
        response = generate_response(user_input)
        st.text_area("Bot:", value=response, height=200)

# Run chatbot in terminal or as a web app
if __name__ == "__main__":
    mode = input("Enter 'web' for Streamlit UI or 'terminal' for console chatbot: ")
    if mode.lower() == "web":
        chatbot_web()
    else:
        chatbot_terminal()







